{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13258069,"sourceType":"datasetVersion","datasetId":8401310}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install deps (quiet mode)\n!pip install geopandas fiona fitz rasterio sentence-transformers chromadb langchain langchain-community transformers -q\n!pip install pymupdf==1.22.5\n!apt-get update\n!apt-get install -y libgdal-dev\n!pip install geopandas rasterio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T13:03:29.309366Z","iopub.execute_input":"2025-10-04T13:03:29.309558Z","iopub.status.idle":"2025-10-04T13:05:33.818236Z","shell.execute_reply.started":"2025-10-04T13:03:29.309541Z","shell.execute_reply":"2025-10-04T13:05:33.817497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\n\ntif_path = \"/kaggle/input/nasa-kenya-urban/GHS_BUILT_S_E2030_GLOBE_R2023A_54009_1000_V1_0_R9_C22.tif\"\n\nwith rasterio.open(tif_path) as src:\n    print(\"TIFF file:\", tif_path)\n    print(\"CRS:\", src.crs)\n    print(\"Bounds:\", src.bounds)\n    print(\"Width x Height:\", src.width, \"x\", src.height)\n    print(\"Number of bands:\", src.count)\n    for i in range(1, src.count + 1):\n        band = src.read(i)\n        print(f\" Band {i}: dtype={band.dtype}, min={band.min()}, max={band.max()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fiona\n\ngdb_path = \"/kaggle/input/nasa-kenya-urban/KEN.gdb\"\n\nlayers = fiona.listlayers(gdb_path)\nprint(\"Layers in GDB:\", layers)\n\n# Example: load first layer\nimport geopandas as gpd\ngdf_gdb = gpd.read_file(gdb_path, layer=layers[0])\nprint(\"Columns:\", gdf_gdb.columns)\nprint(\"First 5 rows:\")\nprint(gdf_gdb.head())\nprint(\"CRS:\", gdf_gdb.crs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import geopandas as gpd\nimport fiona\nfrom shapely.geometry import Point\n\n# Check layers in the GeoPackage\nprint(\"Layers in KEN.gpkg:\", fiona.listlayers('/kaggle/input/nasa-kenya-urban/KEN.gpkg'))\n\n# Load the GeoDataFrame\ngdf = gpd.read_file('/kaggle/input/nasa-kenya-urban/KEN.gpkg')\nprint(\"Columns:\", gdf.columns)\nprint(\"First 5 rows:\")\nprint(gdf[['PIXELID', 'UrbanCenter', 'COUNTRY', 'geometry']].head())\nprint(\"CRS:\", gdf.crs)\n\n# Check unique values in potential columns\nprint(\"Unique values in 'UrbanCenter':\", gdf['UrbanCenter'].unique())\nprint(\"Unique values in 'COUNTRY':\", gdf['COUNTRY'].unique())\n\n# Search for 'Nairobi' in all string columns\nfor col in gdf.select_dtypes(include=['object']).columns:\n    try:\n        nairobi_rows = gdf[gdf[col].str.contains('Nairobi', case=False, na=False)]\n        if not nairobi_rows.empty:\n            print(f\"Found 'Nairobi' in column '{col}':\")\n            print(nairobi_rows)\n    except Exception as e:\n        print(f\"Could not search in column '{col}': {e}\")\n\n# Try filtering by Nairobi's coordinates\nnairobi_point = Point(36.82, -1.28)\nnairobi_rows = gdf[gdf.geometry.contains(nairobi_point)]\nif not nairobi_rows.empty:\n    print(\"Rows containing Nairobi's coordinates:\")\n    print(nairobi_rows)\nelse:\n    print(\"No rows found containing Nairobi's coordinates.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom typing import List, Dict\nimport pandas as pd\nimport zipfile\nimport gc\nfrom shapely.geometry import box\n\n# Configuration\nDATA_DIR = \"/kaggle/input/nasa-kenya-urban\"\nCHROMA_PATH = \"/kaggle/working/chroma_db\"\nMODEL_NAME = \"all-mpnet-base-v2\"  # 768 dimensions; change to \"all-MiniLM-L6-v2\" for 384 dimensions if needed\nBATCH_SIZE = 5000  # Below Chroma's max batch size (5461)\n\n# Initialize embedding model\ntry:\n    embedding_model = SentenceTransformer(MODEL_NAME)\nexcept Exception as e:\n    print(f\"Error loading SentenceTransformer: {e}\")\n    raise\n\n# Load vector data for spatial metadata assignment\ndef load_vector_data() -> gpd.GeoDataFrame:\n    vector_files = [f for f in os.listdir(DATA_DIR) if f.endswith(('.gpkg', '.gdb'))]\n    gdfs = []\n    for f in vector_files:\n        fpath = os.path.join(DATA_DIR, f)\n        try:\n            gdf = gpd.read_file(fpath, layer=\"KEN_projections\")\n            gdfs.append(gdf)\n        except Exception as e:\n            print(f\"Error reading {fpath}: {e}\")\n    if gdfs:\n        return gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n    return None\n\n# Helper function to assign urban center based on spatial overlap\ndef get_urban_center_from_bounds(bounds, vector_gdf: gpd.GeoDataFrame) -> str:\n    if vector_gdf is None:\n        return \"unknown\"\n    try:\n        tiff_bbox = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n        vector_gdf = vector_gdf.to_crs(\"ESRI:54009\") if vector_gdf.crs != \"ESRI:54009\" else vector_gdf\n        intersects = vector_gdf[vector_gdf.geometry.intersects(tiff_bbox)]\n        if not intersects.empty:\n            urban_centers = intersects[\"UrbanCenter\"].value_counts()\n            return urban_centers.index[0] if not urban_centers.empty else \"unknown\"\n        return \"unknown\"\n    except Exception as e:\n        print(f\"Error in spatial overlap: {e}\")\n        return \"unknown\"\n\n# Processing functions\ndef process_geopkg_gdb(file_path: str) -> List[Dict]:\n    try:\n        gdf = gpd.read_file(file_path, layer=\"KEN_projections\")\n        docs = []\n        for idx, row in gdf.iterrows():\n            geom_desc = f\"Geometry: {row.geometry.type}, Area: {row.geometry.area:.2f} sqm\"\n            attrs = \" | \".join([f\"{k}:{v}\" for k, v in row.items() if pd.notna(v) and k != \"geometry\"])\n            urban_center = str(row.get(\"UrbanCenter\", \"unknown\"))\n            country = str(row.get(\"COUNTRY\", \"unknown\"))\n            anthrome = str(row.get(\"ANTHROME\", \"unknown\"))\n            farm_sys = str(row.get(\"Farm_Sys\", \"unknown\"))\n            doc = f\"{geom_desc} | Attributes: {attrs}\"\n            metadata = {\n                \"file\": file_path,\n                \"type\": \"vector\",\n                \"geometry\": str(row.geometry),\n                \"urban_center\": urban_center,\n                \"country\": country,\n                \"anthrome\": anthrome,\n                \"farm_sys\": farm_sys\n            }\n            docs.append({\n                \"id\": f\"{os.path.basename(file_path)}_{idx}\",\n                \"text\": doc,\n                \"metadata\": metadata\n            })\n        print(f\"Processed {file_path}: {len(docs)} documents, urban_centers: {set(d['metadata']['urban_center'] for d in docs)}\")\n        return docs\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return []\n\ndef process_tif(file_path: str, vector_gdf: gpd.GeoDataFrame) -> List[Dict]:\n    try:\n        with rasterio.open(file_path) as src:\n            data = src.read(1)\n            mean_val = np.mean(data[data > 0]) if np.any(data > 0) else 0.0\n            doc = f\"Raster stats: Mean {mean_val:.2f}, Shape {src.shape}, CRS {src.crs}, Min {src.profile['nodata'] or 0}, Max {data.max()}\"\n            urban_center = get_urban_center_from_bounds(src.bounds, vector_gdf)\n            return [{\n                \"id\": os.path.basename(file_path),\n                \"text\": doc,\n                \"metadata\": {\n                    \"file\": file_path,\n                    \"type\": \"raster\",\n                    \"bounds\": str(src.bounds),\n                    \"urban_center\": urban_center,\n                    \"country\": \"KEN\",\n                    \"anthrome\": \"unknown\",\n                    \"farm_sys\": \"unknown\"\n                }\n            }]\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return []\n\n# Ingest to Chroma with batch processing\ndef ingest_to_chroma(docs: List[Dict]):\n    try:\n        client = chromadb.PersistentClient(path=CHROMA_PATH)\n        # Delete existing collection to avoid dimension mismatch\n        try:\n            client.delete_collection(name=\"nasa_kenya_urban\")\n            print(\"Deleted existing collection to ensure correct embedding dimension.\")\n        except:\n            pass  # Collection may not exist\n        collection = client.create_collection(name=\"nasa_kenya_urban\")\n        \n        # Debug metadata distribution\n        urban_centers = set(d[\"metadata\"][\"urban_center\"] for d in docs)\n        types = set(d[\"metadata\"][\"type\"] for d in docs)\n        print(f\"Documents to ingest: {len(docs)}\")\n        print(f\"Urban centers: {urban_centers}\")\n        print(f\"Document types: {types}\")\n        \n        # Process documents in batches\n        total_docs = len(docs)\n        for i in range(0, total_docs, BATCH_SIZE):\n            batch_docs = docs[i:i + BATCH_SIZE]\n            texts = [d[\"text\"] for d in batch_docs]\n            embeddings = embedding_model.encode(texts, show_progress_bar=True).tolist()\n            ids = [d[\"id\"] for d in batch_docs]\n            metadatas = [d[\"metadata\"] for d in batch_docs]\n            collection.add(embeddings=embeddings, documents=texts, metadatas=metadatas, ids=ids)\n            print(f\"Ingested batch {i // BATCH_SIZE + 1}: {len(batch_docs)} documents\")\n            gc.collect()\n        \n        print(f\"Ingested {total_docs} chunks into Chroma.\")\n        return collection\n    except Exception as e:\n        print(f\"Error ingesting to Chroma: {e}\")\n        return None\n\n# Main execution\nif __name__ == \"__main__\":\n    # Ensure output directory exists\n    os.makedirs(CHROMA_PATH, exist_ok=True)\n    \n    # Check if DATA_DIR exists\n    if not os.path.exists(DATA_DIR):\n        print(f\"Error: Dataset directory {DATA_DIR} not found.\")\n        print(\"Available datasets:\")\n        print(os.listdir(\"/kaggle/input\"))\n        raise FileNotFoundError(f\"Dataset directory {DATA_DIR} not found.\")\n    \n    # Load vector data for TIFF metadata\n    vector_gdf = load_vector_data()\n    \n    # Process and ingest data\n    all_docs = []\n    for file in os.listdir(DATA_DIR):\n        fpath = os.path.join(DATA_DIR, file)\n        if file.endswith(('.gpkg', '.gdb')):\n            all_docs.extend(process_geopkg_gdb(fpath))\n        elif file.endswith('.tif'):\n            all_docs.extend(process_tif(fpath, vector_gdf))\n        gc.collect()\n\n    if all_docs:\n        collection = ingest_to_chroma(all_docs)\n        if collection:\n            # Zip Chroma DB\n            zip_path = '/kaggle/working/chroma_db.zip'\n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                for root, _, files in os.walk(CHROMA_PATH):\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        arcname = os.path.relpath(file_path, CHROMA_PATH)\n                        zipf.write(file_path, arcname)\n            print(f\"Chroma DB zipped at {zip_path}\")\n        else:\n            print(\"Failed to ingest data into Chroma.\")\n    else:\n        print(\"No documents processed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T13:05:55.319209Z","iopub.execute_input":"2025-10-04T13:05:55.319972Z","iopub.status.idle":"2025-10-04T13:10:09.838272Z","shell.execute_reply.started":"2025-10-04T13:05:55.319941Z","shell.execute_reply":"2025-10-04T13:10:09.837618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T13:10:44.238458Z","iopub.execute_input":"2025-10-04T13:10:44.239086Z","iopub.status.idle":"2025-10-04T13:10:44.429105Z","shell.execute_reply.started":"2025-10-04T13:10:44.239061Z","shell.execute_reply":"2025-10-04T13:10:44.428183Z"}},"outputs":[],"execution_count":null}]}